# Why Snapshot Testing for TransFS?

## The Problem

TransFS is a complex virtual filesystem that transforms archive content into various platform-specific layouts. As development progresses, patches and new features risk introducing **breaking changes** that:

1. Remove expected files or directories
2. Change file naming conventions
3. Break emulator compatibility
4. Cause regressions in previously working systems

**Manual verification is impractical** given:
- 7+ different systems (Acorn, Amstrad, MITS, Tandy, etc.)
- Multiple file formats per system (DSK, HDF, VHD, MMB, UEF, etc.)
- Hundreds of files across the virtual filesystem
- Docker-based deployment with volume mappings

## Alternative Approaches (and Why They Fall Short)

### âŒ Manual Testing

```
Approach: Manually check directory listings after each change
```

**Cons:**
- â±ï¸ Time-consuming (hours per change)
- ðŸ‘¤ Human error prone
- ðŸ“ No documentation of expected state
- ðŸ”„ Not repeatable across developers
- ðŸš« Can't run in CI/CD
- ðŸ˜« Developer fatigue leads to skipped testing

**Verdict:** Not sustainable for ongoing development

---

### âŒ Integration Tests with Hardcoded Assertions

```python
def test_archimedes():
    assert os.path.exists("/mnt/transfs/Native/Acorn/Archimedes")
    assert os.path.exists("/mnt/transfs/Native/Acorn/Archimedes/Software")
    assert os.path.exists("/mnt/transfs/Native/Acorn/Archimedes/Software/BIOS")
    # ... hundreds more assertions ...
```

**Cons:**
- ðŸ“œ Verbose and hard to maintain
- ðŸ”§ Brittle - breaks easily with legitimate changes
- ðŸŒ Slow to write
- ðŸ‘ï¸ Hard to see what changed when tests fail
- ðŸ“ˆ Scales poorly (need assertion per file/directory)

**Verdict:** Too much maintenance burden

---

### âŒ Hash-Based File Verification

```python
def test_filesystem_hash():
    current_hash = hash_directory_tree("/mnt/transfs")
    assert current_hash == "abc123..."
```

**Cons:**
- ðŸ•µï¸ No visibility into **what** changed
- ðŸ” Debugging requires manual investigation
- ðŸ“Š Can't tell if change is in 1 file or 100 files
- ðŸ¤· No diff to review

**Verdict:** Fails fast but provides no useful information

---

### âŒ Custom Comparison Scripts

```python
def compare_directories(expected, actual):
    # Custom logic to compare...
    # 50+ lines of code...
```

**Cons:**
- ðŸ› Need to write and maintain comparison logic
- ðŸ§ª Comparison logic itself needs testing
- ðŸ”„ Reinventing the wheel
- ðŸ“š No standard format for diffs

**Verdict:** Unnecessary complexity

---

## âœ… Our Solution: pytest + syrupy Snapshot Testing

```python
def test_transfs_structure(transfs_volume, filesystem_walker, snapshot):
    state = filesystem_walker(transfs_volume)
    assert state == snapshot
```

**Pros:**
- âœ… **Automatic change detection** - Any difference is caught immediately
- âœ… **Clear diffs** - See exactly what changed (added/removed/modified)
- âœ… **Self-documenting** - Snapshots serve as documentation
- âœ… **Easy updates** - `pytest --snapshot-update` when changes are intentional
- âœ… **Fast** - Tests run in seconds
- âœ… **Maintainable** - Minimal code to maintain
- âœ… **CI/CD ready** - Integrates seamlessly
- âœ… **Industry standard** - Well-tested tooling (pytest, syrupy)
- âœ… **Granular** - Separate snapshots per test/system
- âœ… **Comprehensive** - Captures entire filesystem state

## Real-World Comparison

### Scenario: Developer adds support for new Amstrad PCW disk format

#### Manual Testing Approach

```
1. Developer makes changes .......................... 30 min
2. Build Docker container ........................... 2 min
3. Mount SMB share .................................. 1 min
4. Manually browse directory tree ................... 10 min
5. Compare against notes from last time ............. 15 min
6. Check each system wasn't affected ................ 20 min
7. Document what changed ............................ 5 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~83 minutes, error-prone, undocumented
```

#### Snapshot Testing Approach

```
1. Developer makes changes .......................... 30 min
2. Run: pytest -v .................................. 15 sec
3. Review diff showing new directories .............. 2 min
4. Run: pytest --snapshot-update .................... 10 sec
5. Commit (code + snapshots) ........................ 1 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~33 minutes, automated, fully documented
```

**Result:** 60% time savings + better quality

---

### Scenario: Accidental breaking change introduced

#### Without Snapshot Testing

```
1. Developer makes "small" fix ...................... 10 min
2. Commit and push .................................. 1 min
3. Another developer notices issue .................. 2 days later
4. Investigation and debugging ...................... 60 min
5. Git bisect to find problematic commit ............ 20 min
6. Revert or fix .................................... 30 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 2+ days to discover, 110+ minutes to fix
Impact: Broken main branch, wasted team time
```

#### With Snapshot Testing

```
1. Developer makes "small" fix ...................... 10 min
2. Run: pytest -v .................................. 15 sec
   âŒ FAILED: 2 files missing from Electron/
3. Developer: "Oops, that's not right!"
4. Fix the issue .................................... 5 min
5. Run: pytest -v .................................. 15 sec
   âœ… PASSED
6. Commit .......................................... 1 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 16 minutes, caught immediately
Impact: Zero broken commits, zero team disruption
```

**Result:** Breaking change caught before commit

---

## Feature Comparison Table

| Feature | Manual | Hardcoded Assertions | Hash-Based | Snapshot Testing |
|---------|--------|---------------------|------------|------------------|
| **Speed** | ðŸŒ Slow | âš¡ Fast | âš¡ Fast | âš¡ Fast |
| **Maintenance** | âŒ High | âŒ High | âœ… Low | âœ… Low |
| **Visibility** | âš ï¸ Medium | âœ… Good | âŒ Poor | âœ… Excellent |
| **CI/CD Ready** | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Scalability** | âŒ Poor | âš ï¸ Medium | âœ… Good | âœ… Excellent |
| **Documentation** | âŒ None | âš ï¸ Implicit | âŒ None | âœ… Self-documenting |
| **Updates** | N/A | âŒ Manual code changes | âŒ Manual hash update | âœ… `--snapshot-update` |
| **Diff Quality** | âŒ None | âš ï¸ Basic | âŒ None | âœ… Detailed |
| **Learning Curve** | âœ… Easy | âš ï¸ Medium | âœ… Easy | âš¡ Easy |
| **Industry Standard** | âŒ No | âš ï¸ Common | âŒ Custom | âœ… Yes |

## Cost-Benefit Analysis

### Initial Investment

| Approach | Setup Time | Code Written | External Dependencies |
|----------|------------|--------------|---------------------|
| Manual | 0 hours | 0 lines | None |
| Hardcoded Assertions | 8-16 hours | 500+ lines | pytest |
| Hash-Based | 2-4 hours | 100 lines | pytest, hashlib |
| **Snapshot Testing** | **2-3 hours** | **~200 lines** | **pytest, syrupy, deepdiff** |

### Ongoing Costs

| Approach | Per Feature | Per Bug Fix | Per System Added |
|----------|-------------|-------------|------------------|
| Manual | 60+ min | 30+ min | 120+ min |
| Hardcoded Assertions | 30 min (update assertions) | 15 min | 60 min |
| Hash-Based | 5 min (update hash) | 5 min | 10 min |
| **Snapshot Testing** | **2 min (review diff)** | **15 sec (just run tests)** | **3 min (add parameterized test)** |

### Return on Investment

```
Break-even point: After ~5-10 changes
ROI after 1 month: ~10 hours saved
ROI after 1 year: ~100+ hours saved + prevented production issues
```

## Real Developer Quotes (Simulated)

### Before Snapshot Testing

> "I'm afraid to touch the transformation logic because I don't know what it might break."
> â€” Developer A

> "It took me 2 hours to verify my change didn't break the 7 supported systems."
> â€” Developer B

> "We had to roll back a release because Electron support broke and we didn't notice."
> â€” Team Lead

### After Snapshot Testing

> "I can make changes confidently knowing tests will catch any issues immediately."
> â€” Developer A

> "Tests run in 15 seconds and tell me exactly what changed. Game changer!"
> â€” Developer B

> "Haven't had a broken release since we added snapshot testing."
> â€” Team Lead

## When NOT to Use Snapshot Testing

Snapshot testing is **not ideal** for:

1. **Dynamic/non-deterministic output** - Timestamps, random IDs, etc.
   - *TransFS doesn't have this problem - filesystem structure is deterministic*

2. **Large binary files** - Snapshots would be huge
   - *We snapshot structure, not file contents*

3. **Frequently changing APIs** - Every change requires snapshot update
   - *TransFS structure changes are rare and intentional*

4. **Performance-critical paths** - Snapshot comparison adds overhead
   - *Our tests run in seconds, acceptable for dev workflow*

**Verdict:** TransFS is an **ideal** use case for snapshot testing!

## Conclusion

For TransFS, snapshot testing with pytest + syrupy provides:

âœ… **Maximum protection** against breaking changes
âœ… **Minimal maintenance** burden
âœ… **Clear visibility** into what changed
âœ… **Fast feedback** loop
âœ… **Industry-standard** tooling
âœ… **Excellent ROI**

The choice is clear: **snapshot testing is the right solution** for detecting breaking changes in TransFS's complex virtual filesystem.

---

## Further Reading

- [Snapshot Testing Best Practices](https://kentcdodds.com/blog/effective-snapshot-testing)
- [Syrupy Documentation](https://github.com/tophat/syrupy)
- [pytest Documentation](https://docs.pytest.org/)
- [When to Use Snapshot Testing](https://jestjs.io/docs/snapshot-testing) (Jest, but principles apply)
